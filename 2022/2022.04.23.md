# 머신러닝 이론 교육 2일차

### 단순 선형 회귀(simple linear regression)
    * 피처의 종류가 한 개인 데이터에 대한 회귀 모델
    * y = w0 + w1x

### 다중 선형 회귀(multiple linear regression)
    * 피처의 종류가 여러 개인 데이터의 대한 회귀
    * y = w0 + w1x1+ …….+wDxD

### 다항 회귀 (polynomial regression)
    * 독립변수(피처)의 차수를 높인 회귀 모델
    * y = w0 + w1x + w2x^2 + wm x^m

Optimal(최적의)이란 뜻은 데이터를 가장 잘 표현한다는 말과 동치로

모델 예측값과 실제 값의 차이가 가장 적은 모델
손실 함수값을 최소로 만드는 모델 파라미터
최소 값으로 만드는 것을 Optimization 이라고 한다.


### 복잡한 함수의 경우

* 다중 선형회귀, 다항 회귀 비선형 함수
* 최소 제곱법으로 해결 어려움(계산 비용이 굉장히 높아서)
* 어떻게 최적의 파라미터를 찾을 수 있을까?

==> 경사하강법(Gradient Descent)
* 손실 함수의 값을 최소화 시키는 방향으로 파라미터를 업데이트

### 경사 하강법(Gradient descent)
    * 함수의 최솟값은 무조건 순간 변화율 0
    * 손실 함수에 대한 미분값이 0이 되는 방향으로 파라미터의 업데이트 방향을 결정

### 슈도 코드(pseudo code)
1. 현재 파라미터에서의 손실 함수에 대한 미분값을 구함
2.  미분값의 반대 방향으로 파라미터 값을 업데이트
3.  미분값이 0이 될 때까지 1~2번을 에폭(epoch)만큼 반복


데이터의 분할

* 입력된 데이터는 학습 데이터와 평가 데이터로 나눌 수 있다.
* 학습 데이터는 모델 학습에 사용되는 모든 데이터셋
* 평가 데이터는 오직 모델의 평가만을 위해 사용되는 데이터셋
* 평가 데이터는 절대로 모델 학습에 사용되면 안된다.

평가 데이터 
* 학습 데이터와 평가 데이터는 같은 분포를 가지는가
* 평가 데이터는 어느 정도 크기를 가져야 하는가


Bias and Variance Trade-off

모델의 복잡도

* 선형에서 비선형 모델로 갈수록, 복잡도가 증가함(파라미터가 증가)
* 모델이 복잡해질수록(에폭이 증가) ,학습 데이터를 더 완벽하게 학습함

문제가 발생하는데 데이터가 많을때 (Under-fitting)
적을 때는 (Over-fitting,과적합) 발생

편향은 under-fitting과 관련 있는 개념
분산은 over-fitting과 관련있는 개념

---------------------------------------------------------------
### 데이터셋

모델 학습의 정도를 검증하기 위한 데이터셋
모델 학습에 직접적으로 참여하지 못함
학습 중간에 계속해서 평가를 하고 가장 좋은 성능의 파라미터를 저장해둠

검증 데이터셋
<br>

크기로 나누면
|			Train data				|  Vaild	data |  Test data | 

Test data 와 Vaild data 차이 : 학습시에 볼수 있는가
