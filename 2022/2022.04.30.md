## 머신러닝 교육



### XGBoost
    • GBM 알고리즘의 성능과 속도 면에서 향상된 알고리즘이다.
    • 기존의 GBM은 학습 데이터에 대한 residual(남는 데이터)을 계속 줄여 over-fitting되기 쉽다.
    • Split finding 알고리즘을 통해 연산의 효율성을 높인다.
    • 정규화 항을 손실 함수에 추가한다.
    • 기존에는 모든 피처를 split 기준으로 탐색했다
    • 이에 대한 근사 알고리즘을 제안해 속도를 향상시킨다.
    
### Light GBM
    • 기존의 boosting 알고리즘은 B번의 반복 학습 때마다 전체 데이터셋을 살펴본다.
    • 이 과정에서 대부분의 계산 비용이 발생한다.
    • 결정 트리 학습에 사용되는 데이터 수를 다음의 방법들로 줄인다.
## Dimension Reduction(차원 축소)

Dimension reduction은 차원축소라고도 하며 많은 피처로 구성된 다차원 데이터 세트의 차원을 축소해 새로운 차원의 데이터 세트를 생성하는 것을 의미한다.

일반적인 차원 축소는 피처 선택과 피처 추출로 나눌 수 있고 피처 선책은 말 그대로 특정 피처에 종속성이 강한 불필요한 피처는 아예 제거하는 것이다.

피처 추출은 기존 피처를 저차원의 중요 피처로 압축해서 추출하는 것으로 

이렇게 새롭게 추출된 중요 피처는 기존의 피처가 압축된 것이므로 기존의 피처와는 완전히 다른 값이 됩니다.

피처 선택의 장점은 선택한 피처의 해석이 용이하다는 점이고 단점은 피처간 상관관계를 고려하기 어렵다는 점입니다.

피처 추출의 장점은 피처 간 상관관계를 고려하기 용이하고 피처의 개수를 많이 줄일 수 있다는 점이고 단점은 추출된 변수의 해석이 어렵다는 점이 있다.

### 차원 축소 (dimension reduction)
    • 데이터에 대한 차원 축소는 속도뿐만 아니라 성능면에서 필요하다.
    • 모델 학습에 불필요한 피처(속도 향상)나 방해되는 피처(성능 향상)를 제거해야한다.
    • 방해되는 피처란, over-fitting 문제를 발생시키는 피처로 이해 가능하다.
    • 이는 차원의 저주와 관련 있는 현상
    
### 차원의 저주 (curse of dimensionality)
    • 차원의 저주란 차원이 증가하면서, 학습 데이터 수(N)가 차원의 수(p)보다 적어져 성능이 저하되는 현상을 의미한다.
    • 차원 내에 존재하는 데이터들이 희박해지는(sparse) 현상이다.
![curse of dimensionality](https://user-images.githubusercontent.com/71219602/166477668-d824bbee-af9e-4d6b-b1ea-236ae7cffbf8.png)

#### 차원의 저주 (curse of dimensionality)
    • 빈 공간이 많아지며, 이는 정보가 없는 공간이 많아지는 것을 의미한다.
    • 해결할 수 있는 방법은 크게 두 가지가 있다.
        1. 데이터를 수집
        2. 차원 축소
    • 차원 축소 방법으로 크게 형상 선택(feature selection)과 형상 추출(featureextraction) 두 가지 사용한다.
