## 머신 러닝 교육

## Bagging

### 앙상블 학습 (ensemble learning)
    • 앙상블 학습은 여러 개의 모델을 학습시켜, 다양한 예측 결과들을 이용하는 방법론
    • 모든 머신 러닝 모델과 회귀, 분류 문제 모두에 적용 가능함
    • 보통 결정 트리에서 자주 사용됨
    • 크게 Bagging과 Boosting 두 가지 방법론이 존재
    
    
### 부트스트랩 (bootstrap)
    • 모수의 분포를 정확히 추정하기 위해선 더 많은 표본 데이터셋이 필요함
    • 하지만 표본을 계속 얻는 것은 현실적으로 어려움
    • 현재 가지고 있는 샘플을 복원 추출(sampling with replacement)하여 여러 개(𝑩개)의 데이터셋을 만듬
    
### 배깅 (Bagging)
    • Bootstrap을 이용해, 한개의 학습 데이터셋으로부터 B개의 데이터셋을 추출
    • 각각의 데이터셋으로<𝒇∗𝒃(𝒙) 모델을 학습함
    • 모든 예측치를 평균(회귀)내거나, majority vote(분류)를 취해 분산 오류를 낮춤
    • 배깅을 사용하면 결정트리의 성능면에서의 단점을 보완 가능하나 합습결과에 대한 해석력은 떨어지는 편이다.

### 랜덤 포레스트 (random forests)
    • Bagged tree 사이의 상관관계를 없애 성능을 향상시킨 알고리즘
    • 원래는 p개의 variable을 모두 고려해 split을 결정해 결정 트리를 학습함
    • 만약 강력한 variable이 있으면, B개의 모든 트리가 top split으로 이를 사용할 것임
    • 상관관계가 커지면 (= high 𝐂𝐨𝐯(<𝒇∗𝒊,<𝒇∗𝒋)), 분산 오류가 크게 줄어들지 못함
    • p개의 variable 중 m개를 랜덤하게 선택해 결정 트리를 학습한다.
    • 상관관계가 줄어든 결정 트리를 사용하기 때문에, 분산 감소 효과가 증폭된다.
