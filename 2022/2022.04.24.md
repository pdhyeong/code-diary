## 머신러닝 교육 3일차

### Regularization

#### 정규화 손실함수

    * 모델의 복잡도가 커진다 == 모델의 파라미터 수가 많아진다.
    * 모델의 복잡도가 커질수록, 과적합(over-fitting)이 발생할 가능성이 커진다.
    * 복잡도가 큰 모델을 정의하고, 그 중 중요한 파라미터만 학습
    * 필요없는 파라미터 값을 0으로 만들것

#### 정규화 종류

Ridge 회귀
Lasso 회귀


Ridge Regression


### Logistic Regression

#### 오즈(odds)
    * 성공(y=1)확률이 실패(y=0) 확률에. 비해 몇 배 더 높은가를 나타냄
    * odds = p(y=1|x)/ 1-p(y=1|x)

#### 베이즈 정리(baye’s theorem)

    * P(w|X) = P(X|w)P(w)/P(X) 비례한다 = P(X|w)P(w)

    * 사후 확률 (posterior , P(w|X)) : 데이터가 주어졌을때 가설에 대한 확률 분포
    * 우도확률 (likelihood,P(X|w)) : 가설을 잘 모르지만 안다고 가정했을 경우, 주어진 데이터의 분포
    * 사전 확률(prior,P(w)): 데이터를 보기 전, 일반적으로 알고 있는 가설의 확률
    * 이 확률들을 통해 가설(모델의 파라미터)를 추정하는 방법으로 MLE와 MAP 두 가지가 있다.


## Support vertor machine


 ### Hyperplane
     • p차원에서의hyperplane은 p−1차원에서의 평평한 어핀공간임
     • 𝒃+𝒘𝟏𝐗𝟏+𝒘𝟐𝐗𝟐+⋯+𝒘𝐩𝐗𝐩=𝐛+ 𝐰,𝐗 =𝟎
     • 𝐰=(𝒘𝟏,𝒘𝟐,⋯,𝐰𝐩)의 normalvector 는 hyperplane과orthogonal방향을의미한다.
     • Ex.2차원에서의hyperplane은선,3차원에서는면 

### Maximal margin classifier

모든 separating hyperplane 중에서 이진 클래스 사이의 gap또는 margin을 최대로 만들어 주는것을 찾음

결정 경계 영향을 미치는 샘플들을 서포트 벡터(support vector) 라고 부른다.

#### 라그랑주 승수법(Lagrange multiplier method)
* 제약식이 있는 최적화 문제를 라그랑주 승수 항을 추가해, 제약이 없는 문제로 바꾸로 방법
