## 머신러닝 교육

## Boosting

## 부스팅 (Boosting)

배깅과 마찬가지로, 다양한 알고리즘과 회귀와 분류 문제에 모두 적용 가능

결정 트리를 사용한 부스팅 알고리즘

1. AdaBoost
2. Gradient Boosting(GBM)
3. XGBoost
4. Light GBM
등 이 있다.

• 배깅은 병렬적으로 생성된 결정 트리를 앙상블하는 방법이고
    
• 부스팅은 이전 스텝의 트리 정보를 활용해 순차적(sequentially) 트리를 만드는 것

### AdaBoost
    • 최초의 부스팅 알고리즘
    • 이전 결정 트리가 잘못 예측한 데이터에 큰 가중치(W.i)를 부여해, 다음 결정 트리가 더 집중할 수 있도록 순차적으로 학습
    • 결정 트리로는 stump 구조 사용
    • stump 구조란 split이 한번인 가장 간단(2 terminal node)인 결정트리

결정 트리 별로 계산된 모델 가중치를 합산해서 최종 모델을 생성한다.

손실함수는 지수 손실의 합으로 정의한다


### Gradient Boosting (GBM)
    • 현재 모델의 오차(residual)를 줄여주는 방향으로 결정 트리를 학습하는 방법론
    • 첫 번째 결정 트리는 하나의 leaf node 구조 (= 전체 데이터의 평균으로 예측)
    • 이 후에는 일반적으로 stump 보다는 더 복잡한 트리 구조를 사용
    • 손실 함수로는 보통 미분 가능한 MSE loss, L1 loss, 혹은 Logistic loss 사용
    
    
강의를 들으면서 이해는 가는데 머리가 슬슬 조아린다. 실습도 문제인...... 
