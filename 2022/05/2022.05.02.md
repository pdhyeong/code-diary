## 머신러닝 교육

### 고유값 분해 (eigen decomposition)
    • 행렬 A를 고유벡터를 열벡터로 하는 행렬과 고유값을 대각원소로 하는 행렬의 곱으로 분해한다.
    • 고유값과 고유벡터는 정방행렬(𝑛×𝑛)에 대해서만 정의된다.
    • 단, 행렬 A가 n개의 일차독립인 고유벡터를 가져야 한다.
    (일차독립: 어느 한 벡터도 다른 벡터들의 일차결합으로 표현 불가능)
    • 대칭 행렬(symmetric matrix)은 항상 고유값 분해 가능하다.
    • 행렬에 대한 고유값은 유일하지만, 고유벡터는 유일하지 않다.
    𝐴𝒗 = 𝜆𝒗, 𝐴(𝒄𝒗) = 𝜆(𝒄𝒗), 𝐴 𝒗𝟏 + 𝒗𝟐 = 𝜆 𝒗𝟏 + 𝒗𝟐 ,⋯
    • 일반적으로 벡터의 크기가 1인 단위벡터를 사용한다.
    
### Principal Component Analysis (주 성분 분석)
    • 전체 데이터의 분포를 가장 잘 설명할 수 있는 주성분을 찾아주는 방법론이다.
    • 이를 데이터의 공분산 행렬에 대한 고유값 분해를 통해 얻는다.
    • 공분산 행렬은 대칭 행렬이기 때문에 고유값 분해가 항상 가능하다.
    • 가장 큰 분산을 가진 방향의 고유벡터로 데이터를 투영시켜 차원을 축소시킨다.
![image](https://user-images.githubusercontent.com/71219602/166820177-eebf62cc-d585-440d-93ce-3e8dec184bd1.png)

ℎi = (||𝑥i|| 𝑐𝑜𝑠𝜃) 𝑣 = (||𝑥i|| ∗ xi*v/(||xi||*||v||)) = (𝑥i o 𝑣) v
• 데이터들의 분산이 최대화되는 방향이란, ℎ4의 크기에 대한 분산이 최대화된 것이다.

(솔직히 이해하기 너무 힘들다.)


### SNE (stochastic neighbor embedding)
우리의 목표는 고차원(N)의 데이터(x) n개를 동일한 개수의 저차원(M)의 데이터(y)로 만들고자 한다.

그리고 차원을 줄이더라도 데이터간의 거리는 원래와 유사하게 하고자 한다.


![image](https://user-images.githubusercontent.com/71219602/166821280-1b4dae82-648d-49da-b70e-81aa2ac9ad32.png)

### t-SNE

차원 축소에 쓰이는 대표적인 알고리즘으로, 고차원 데이터의 시각화에 용이하게 쓰인다.

각 데이터 포인트 주변으로 유사도를 구해 클러스터를 만든다. 

관심 데이터 포인트를 하나 잡고, 그 값을 평균으로 갖는 정규분포그래프를 그린다.

주변의 데이터의 경우(같은 클러스터) 유사도가 높게 나오고,(정규분포의 확률값) 먼 데이터는 유사도가 낮음.

    • 가우시안 분포는 확률 값 변화의 경사가 가파른 특성이 있다.
    • 따라서, 일정 거리 이상부터는 값이 매우 작아지는 crowding 문제가 있다.
    • 먼 거리의 샘플에 대한 가중치 값이 의미를 잃어버리는 문제가 발생한다.
    • 가우시안 분포를 t 분포로 대체해 사용한 것이 t-SNE 알고리즘이다.
    
    
으아 대충 이해는 되는건 여기까지 인데 하루남았으니까 힘내자...... 너무 어렵다

참고 : https://dowhati1.tistory.com/11
