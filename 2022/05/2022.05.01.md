## 머신러닝 교육


### 주성분 분석 (PCA)
    • 전체 데이터의 분포를 가장 잘 설명할 수 있는 주성분을 찾아주는 방법론
    • 주성분이란 그 방향으로 데이터들의 분산이 가장 큰 방향벡터를 의미함
    • 분산이 가장 크다는 말은 데이터가 가장 넓게 퍼져있어 구분짓기 쉽다는 뜻임
    • 주성분 방향벡터를 찾아 데이터를 그 위로 투영(projection)시켜 차원을 한 단계 낮춤
![image](https://user-images.githubusercontent.com/71219602/166478292-4b54e9b3-35d6-481f-8f6c-2158b50a17f5.png)


### 공분산 (covariance)
공분산이란 2개의 확률변수의 선형 관계를 나타내는 값이다.[1] 만약 2개의 변수중 하나의 값이 상승하는 경향을 보일 때 

다른 값도 상승하는 선형 상관성이 있다면 양수의 공분산을 가지고 반대로 2개의 변수중 하나의 값이 상승하는 경향을 보일 때 

다른 값이 하강하는 선형 상관성을 보인다면 공분산의 값은 음수가 된다.

    • 두 변수 𝑥, 𝑦의 공분산은 다음과 같이 정의된다.   
        𝒄𝒐𝒗 𝒙, 𝒚 = 𝜠 𝒙 − ,𝒙 𝒚 − ,𝒚 = 𝐄 𝐱𝐲 − ,𝒙,𝒚
    • 한 쪽 변수의 값이 커짐에 따라 다른 변수 또한 커지면 양의 상관관계를, 작아지면 음의 상관관계를 갖음
    • 서로 상관관계가 없는 독립적인 관계에서는 공분산이 0이 됨
    
### 공분산 행렬 (covariance matrix)
    • 각각의 데이터 피처들 사이의 공분산 값을 원소로 하는 행렬
    • 𝐶 = Ε[(x − 𝑥̅) (𝑥 − 𝑥̅)^T] ∶ N×N행렬
    • 열에 대한 부분이 피처가 되며, 피처 사이의 공분산이 원소가 된다.
    • 공분산 행렬은 대칭(symmetric) 성질을 만족한다.
    
    
### 고유값과 고유벡터(eigenvalue & eigenvertor)
    • 행렬 A를 선형변환으로 봤을 때, 선형변환 A 이후에도 자기 자신의 상수배가 되는 벡터를 고유벡터(eigenvector)라 부르고 
      이 때의 상수배 값을 고유값(eigenvalue)라 한다.
    • 𝐴𝒗 = 𝜆𝒗
    • 즉, 선형변환 A에 의해 방향은 보존되고 스케일만 변환되는 방향 벡터와 스케일 값을 의미한다. 
    
고유값, 고유벡터는 정방행렬의 대각화와 밀접한 관련이 있다 (eigendecomposition은 정방행렬에 대해서만 가능하다.)

먼저 대각행렬과의 행렬곱에 대해 살펴보면, 대각행렬을 뒤에 곱하면 행렬의 열벡터들이 대각원소의 크기만큼 상수배가 된다.

이제 선형대수학 부분까지 나오니까 좀많이 어지럽다. 큰일이다. 다까먹어서 벡터 기초 부터 다시 봐야될판이다.

참고 : https://ko.wikipedia.org/wiki/%EA%B3%B5%EB%B6%84%EC%82%B0
