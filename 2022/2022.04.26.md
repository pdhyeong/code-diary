## 머신러닝 교육

## Duality gap
    * 𝒇∗ ≥ 𝐦𝐢𝐧
    * 𝑳 𝒙, 𝒖, 𝒗 = 𝒈(𝒖, 𝒗)
    * 𝑔(𝑢, 𝑣)를 최대화하는 것은 원초 문제의 최적값과 가까워지는 것
    * 둘 사이의 gap이 존재하면 weak dual, 존재하지 않으면 strong dual (𝑓∗ = 𝑔(𝑢, 𝑣))

라그랑주 듀얼문제(largrange dual problem)
* 최소화 문제를 최대화 문제로 바꾸어 푸는것.

## Slater condition

가정 즉 조건

* 조건 1: Primal problem이 convex(convex = 두 포인트에 중간 값이 영점의 값보다 언제나 크거나 같다라는 조건 상황)
(i.e., f and h!,⋯, hi are convex, l!,⋯, lj are affine) affine = 선형 공간에서의 위치변환

* 조건 2 : There exists at least one strictly feasible x ∈ ℝE
(i.e., h! x < 0,⋯, hi x < 0 and l! x = 0,⋯, lj x = 0)

* 조건 1과 2를 만족하면 strong duality를 만족함

## Hyperparameter 𝑪 in SVM

* 𝐂의 값이 커지면:
 
      1. ϵ의 영향이 커져 0으로 보내며, 마진의 폭이 줄어듬
      2. Support vector 수가 줄어들어, 적은 샘플로 결정 경계를 찾음
      3. Bias ↓(편향 감소), Variance ↑(분산 증가)
      
* 𝐂의 값이 작으면:

      1. 마진의 폭이 커짐
      2. 모든 샘플이 support vector가 됨
      3. Bias ↑(편향 증가), Variance ↓(분산 감소)

수리통계학 내용이나 수학 기본 내용등은 필기노트에 적는중인데 정말

취미로 듣게 된거지만 뒤로 갈수록 내용이 점점 어려워지넹.....
